{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models: Random Forests and Boosting Lab\n",
    "\n",
    "## Retail Customer Churn Model\n",
    "\n",
    "As a junior data scientist at RetailTech Solutions, a retail analytics firm, you've been assigned to help a major e-commerce client predict customer churn. The client has noticed an increasing trend of customers abandoning their platform, despite competitive pricing and a wide product range.\n",
    "\n",
    "Your team lead has provided you with historical customer data containing various metrics including usage patterns, customer support interactions, and account details. Your task is to build several ensemble models to predict which customers are at risk of churning, and identify the key factors driving customer departures.\n",
    "\n",
    "The VP of Customer Experience will use your insights to develop targeted retention strategies, so both prediction accuracy and model interpretability are important. You'll apply the ensemble learning techniques you've learned, specifically Random Forest and Boosting, to address this business challenge.\n",
    "\n",
    "## Modelling Process for Lab:\n",
    "- Data Exploration\n",
    "- Data Setup\n",
    "- Baseline Model (Random Forest)\n",
    "- Boosting Models\n",
    "- Hyperparameter Tuning\n",
    "- Feature Importance\n",
    "\n",
    "## Data Overview\n",
    "Data File: ecommerce_customer_data.csv\n",
    "\n",
    "This dataset contains 15,000 customer records with 14 features and the churn target variable.\n",
    "\n",
    "Contains columns:\n",
    "- account_age_months: Number of months since customer account creation (numeric)\n",
    "- avg_orders_per_month: Average number of orders placed monthly (numeric)\n",
    "- avg_order_value: Average dollar amount spent per order (numeric)\n",
    "- returns_rate: Proportion of items returned from total orders (numeric, 0-1)\n",
    "- support_tickets_6m: Number of customer support tickets in last 6 months (integer)\n",
    "- reviews_submitted: Total number of product reviews submitted (integer)\n",
    "- website_visits_per_month: Average website visits per month (integer)\n",
    "- cart_abandonment_rate: Proportion of shopping carts abandoned (numeric, 0-1)\n",
    "- loyalty_member: Whether customer joined loyalty program (binary: 0=No, 1=Yes)\n",
    "- payment_failures_12m: Number of payment failures in last 12 months (integer)\n",
    "- device_type: Primary device used for shopping (ordinal: 1=Mobile, 2=Mixed, 3=Desktop)\n",
    "- discount_usage_rate: Proportion of orders using discount codes (numeric, 0-1)\n",
    "- days_since_last_active: Number of days since last website activity (integer)\n",
    "- satisfaction_score: Customer satisfaction rating (ordinal: 1-10)\n",
    "- churn: Target variable indicating customer has left (binary: 0=Retained, 1=Churned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup - Import Libraries and Load Data\n",
    "\n",
    "First, let's import all the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step0\n",
    "# Run this cell without changes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the e-commerce customer data\n",
    "df_ecom = pd.read_csv('ecommerce_customer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration\n",
    "\n",
    "In the first part here you are tasked with performing some basic EDA to investigate your data (features and target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "df_ecom.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step1\n",
    "# Investigate the class distribution of churn via value_counts and visualization\n",
    "churn_counts = None\n",
    "\n",
    "# Visualize (use the counts object)\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# Code for plot provided\n",
    "sns.barplot(x=X, y=y)\n",
    "plt.xlabel('Churned')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution of Churn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step2\n",
    "# Produce a correlation matrix using pandas to visualize potential important features\n",
    "# Show correlations with churn (subset the matrix to just churn column\n",
    "correlations = None\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Setup\n",
    "\n",
    "You need to prepare the data for modeling. The data provided is already processed and cleaned for the sake of this lab (categorical variables encoded). Seperate your data into X features and y target and then perform a train test split.\n",
    "- Set random_state = 42\n",
    "- Ensure an 80-20 split (train-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step3\n",
    "# Seperate data into X and y - use all features\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# Split data using sklearn, follow the standard naming conventions (X_train, X_test etc...)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Baseline Random Forest Model\n",
    "You need to instansiate and train (fit) an untuned random forest classifier and evaluate it using cross-validation. Use the default score of accuracy.\n",
    "- Set random_state = 42 inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step4\n",
    "# Instanstiate model and fit the model\n",
    "rf_model = None\n",
    "\n",
    "# Get training score\n",
    "rf_train_score = None\n",
    "\n",
    "# Cross validation scores (don't average)\n",
    "rf_cv_scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes to display results\n",
    "print(f\"Random Forest Training Score: {rf_train_score:.3f}\")\n",
    "print(f\"Random Forest CV Score: {rf_cv_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Boosting Models\n",
    "In this section you will iterate on your modelling approach to investigate the performance of various untuned boosting models.\n",
    "- Use random_state = 42 for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step5\n",
    "# Instantiate and fit all models\n",
    "# Adaboost model\n",
    "ada_model = None\n",
    "\n",
    "# Gradient Boosting model\n",
    "grad_model = None\n",
    "\n",
    "# XBGboost model\n",
    "xgb_model = None\n",
    "\n",
    "# Get training scores\n",
    "ada_train_score =  None\n",
    "grad_train_score = None\n",
    "xgb_train_score = None\n",
    "\n",
    "# Cross validate all models using accuracy (don't average the scores)\n",
    "ada_cv_scores = None\n",
    "grad_cv_scores = None\n",
    "xgb_cv_scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes to dsiplay results\n",
    "print(f\"Training and Cross Validation Performance Comparison of Boosted Models\")\n",
    "print(f\"Adaptive Boosting: Train - {ada_train_score:.3f}, CV - {ada_cv_scores.mean():.3f}\")\n",
    "print(f\"Gradient Boosting: Train - {grad_train_score:.3f}, CV - {grad_cv_scores.mean():.3f}\")\n",
    "print(f\"Extreme Gradient Boosting: Train - {xgb_train_score:.3f}, CV - {xgb_cv_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Hyperparameter Tuning\n",
    "\n",
    "Based on the results above you want to select the model that has the most room for improvement (is overfitting with highest train score) and attempt to optimize the model via a targeted Grid Search. Utilize the provided hyperparameters and values for your grid.\n",
    "- 'learning_rate': [0.05, 0.1]\n",
    "- 'n_estimators': [200, 300]\n",
    "- 'max_depth': [3, 5]\n",
    "- 'min_child_weight': [1, 5]\n",
    "- 'scale_pos_weight': [1, 3]\n",
    "\n",
    "NOTE: You should expect this grid search to take a minute or two to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step6\n",
    "# Assign the model object\n",
    "gs_model = None\n",
    "\n",
    "# Create Param Grid\n",
    "param_grid = None\n",
    "\n",
    "# Instantiate GridSearchCV object\n",
    "grid_search = None\n",
    "\n",
    "# Perform the grid search (fit)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes to display results\n",
    "print(\"Best Model Results from Grid Search\")\n",
    "print(f\"CV Score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Final Model Analysis\n",
    "\n",
    "For the sake of timing we will stop at one grid search. In practice (especially with advanced boosting models) multiple searchs are probably warranted, this grid search only touches a few of the most important hyperparameters involved. Treat the best estimator from the grid search as your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step7\n",
    "# Extract final model\n",
    "final_model = None\n",
    "\n",
    "# Final Model training accurary\n",
    "final_score_train = None\n",
    "\n",
    "# Final model testing accuracy\n",
    "final_score_test = None\n",
    "\n",
    "# Produce classificaiton report\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Produce confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes to display results\n",
    "print(f\"Final Model Evaluation\")\n",
    "print(f\"Accuracy on the Training Data: {final_score_train:.3f}\")\n",
    "print(f\"Accuracy on the Testing Data: {final_score_test:.3f}\")\n",
    "print(f\"Classification Report\")\n",
    "print(cr)\n",
    "print(f\"Confusion Matrix\")\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your boss specifically wanted a model with high accuracy and interpretability which you have achieved! However, based on the results above and what you know about churn and business context, what might be a good alternative metric to try and optimize for? \n",
    "\n",
    "Select from one of the options below:\n",
    "- recall\n",
    "- f1_score\n",
    "- precision\n",
    "- roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step8\n",
    "# Assign name of metric as string\n",
    "alternative_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step9\n",
    "# Extract feature importance from final model\n",
    "feature_importance = None\n",
    "\n",
    "importances = pd.Series(feature_importance, index=X_train.columns)\n",
    "importances = importances.sort_values(ascending=False)\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "importances.plot(kind='bar')\n",
    "plt.title('Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cohort_Env)",
   "language": "python",
   "name": "cohort_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
